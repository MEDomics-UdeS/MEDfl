{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf489a3",
   "metadata": {},
   "source": [
    "# Database and DataSets Management with **MedFL**\n",
    "\n",
    "@Author : [MEDomics consortium](https://github.com/medomics/)\n",
    "\n",
    "@Email : medomics.info@gmail.com\n",
    "\n",
    "### Introduction\n",
    "One of the main advantages of *MedFl* over the *Flower* package is that MedFl utilizes a database to store all the steps of the learning process, configurations, and learning results.\n",
    "\n",
    "1. Store network elements of the federated learning architecture (Network, Nodes, Server)\n",
    "2. Store the initial DataSet and the Federated DataSet\n",
    "3. Store the FL Pipeline\n",
    "4. Store the results of the training and testing\n",
    "\n",
    "The database will assist researchers in analyzing and comparing different results based on various configurations, enabling them to track and select the best configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d6d64",
   "metadata": {},
   "source": [
    "In this tutorial, we'll demonstrate how to initialize your database and establish its connection to Medfl. Subsequently, we'll explore the step-by-step process of storing various pieces of information.\n",
    "\n",
    "Our choice for utilizing [MySQL](https://www.mysql.com/fr/) as the database system is due to its robust features, reliability, and widespread adoption in the industry. Its strong support for structured query language (SQL) and its scalability make it an ideal choice for managing the diverse data sets and configurations within Medfl.\n",
    "\n",
    "<img src=\"../Images/logos/mysqllogo.png\"  style=\"width:150px ;height:50px ;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb0079",
   "metadata": {},
   "source": [
    "Before beginning, ensure that you have installed MySQL and one of the servers, such as [WAMP](https://www.wampserver.com) or [XAMPP](https://www.apachefriends.org/fr/index.html), and have them running. \n",
    "\n",
    "To visualize your database, you can open [PHPMyAdmin](https://www.phpmyadmin.net) , a web-based tool that allows for convenient management and visualization of your database.\n",
    "\n",
    "<img src=\"../Images/logos/wampLogo.png\"  style=\"width:120px ;height:50px ;\"> \n",
    "<img src=\"../Images/logos/xampplogo.png\"  style=\"width:160px ;height:50px ;\"> \n",
    "<img src=\"../Images/logos/phpmyadmin.png\"  style=\"width:150px ;height:50px ;\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '../..'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc8ae1",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "\n",
    "## MedFL Imports \n",
    "from Medfl.NetManager.node import Node\n",
    "from Medfl.NetManager.network import Network\n",
    "from Medfl.NetManager.dataset import DataSet\n",
    "from Medfl.NetManager.flsetup import FLsetup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b771ca",
   "metadata": {},
   "source": [
    "### Create The DataBase\n",
    "\n",
    "to create the databse we need to execute a script\n",
    "\n",
    "<code>!python ../../scripts/create_db.py</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b0d1d",
   "metadata": {},
   "source": [
    "Now we will see what this script do step by step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02183b82",
   "metadata": {},
   "source": [
    "1. Connect to MySQL \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to mysql ( specify your user and password )\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"\"\n",
    ")\n",
    "\n",
    "# initialiser the cursor \n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "print(mycursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d66b9",
   "metadata": {},
   "source": [
    "2. Create the MedFL DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'MEDfl' database if it doesn't exist\n",
    "mycursor.execute(\"CREATE DATABASE IF NOT EXISTS MEDfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10704345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'MEDfl' database\n",
    "mycursor.execute(\"USE MEDfl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c75355",
   "metadata": {},
   "source": [
    "3. Get All Tables\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all tables in the database\n",
    "mycursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "tables = mycursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f72704",
   "metadata": {},
   "source": [
    "4. Drop all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93467747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop each table one by one\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    mycursor.execute(f\"DROP TABLE {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbfcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = mycursor.fetchall()\n",
    "\n",
    "for table in tables:\n",
    "    print(table)  # Display the table names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f38de1",
   "metadata": {},
   "source": [
    "3. Create Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da1443",
   "metadata": {},
   "source": [
    "#### Network Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Networks table\n",
    "mycursor.execute(\n",
    "        \"CREATE TABLE Networks( \\\n",
    "                     NetId INT NOT NULL AUTO_INCREMENT, \\\n",
    "                     NetName VARCHAR(255), \\\n",
    "                     PRIMARY KEY (NetId) \\\n",
    "                     );\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a01f0",
   "metadata": {},
   "source": [
    "#### Nodes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Nodes table\n",
    "mycursor.execute(\"CREATE TABLE Nodes ( \\\n",
    "     NodeId int NOT NULL AUTO_INCREMENT,\\\n",
    "     NodeName varchar(255) DEFAULT NULL,\\\n",
    "     train tinyint(1) DEFAULT '1',\\\n",
    "     NetId int DEFAULT NULL,\\\n",
    "     PRIMARY KEY (NodeId),\\\n",
    "     KEY net_id (NetId),\\\n",
    "     CONSTRAINT net_id FOREIGN KEY (NetId) REFERENCES Networks (NetId) ON DELETE SET NULL ON UPDATE SET NULL\\\n",
    "    )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7cdc5",
   "metadata": {},
   "source": [
    "### Creating a DataSet Table\n",
    "In order to create a DataSet Table, we require a CSV file containing our DataSet. Extracting column names from the CSV file enables us to replicate the same columns within our database.\n",
    "\n",
    "For this demonstration, we will use the *sapsii_score* DataSet from the *EiCU*, specifically the `sapsii_score_knnimputed_eicu.csv` file. This CSV file will serve as the basis for defining the columns in our database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e048b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose your Dataset File \n",
    "data_df = pd.read_csv('../sapsii_score_knnimputed_eicu.csv')\n",
    "\n",
    "columns = data_df.columns.tolist()\n",
    "\n",
    "\n",
    "column_map = {\"object\": 'VARCHAR(255)' , 'int64' : 'INT', 'float64':'FLOAT'}\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abac1f",
   "metadata": {},
   "source": [
    "Create the subQuery based on the dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_query = \"\".join(f\"{col} {column_map[str(data_df[col].dtype)]},\" for col in columns)\n",
    "sub_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset table\n",
    "mycursor.execute(f\"CREATE TABLE DataSets( \\\n",
    "                 DataSetId INT NOT NULL AUTO_INCREMENT, \\\n",
    "                 DataSetName VARCHAR(255), \\\n",
    "                 NodeId INT,\\\n",
    "                 {sub_query}\\\n",
    "                 PRIMARY KEY (DataSetId), \\\n",
    "                 FOREIGN KEY (NodeId) REFERENCES Nodes(NodeId)\\\n",
    "                 )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdbe67",
   "metadata": {},
   "source": [
    "Create the rest of the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FLsetup table\n",
    "mycursor.execute(\"CREATE TABLE FLsetup (\\\n",
    "FLsetupId int NOT NULL AUTO_INCREMENT,\\\n",
    "name varchar(255)  NOT NULL, \\\n",
    "description varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL,\\\n",
    "creation_date datetime NOT NULL,\\\n",
    "NetId int NOT NULL,\\\n",
    "column_name varchar(255) DEFAULT NULL,\\\n",
    "PRIMARY KEY (`FLsetupId`) \\\n",
    ")\")\n",
    "\n",
    "# Create FLpipeline table\n",
    "mycursor.execute(\"CREATE TABLE FLpipeline(\\\n",
    "    id int NOT NULL AUTO_INCREMENT,\\\n",
    "    name varchar(255) NOT NULL, \\\n",
    "    description varchar(255) NOT NULL,\\\n",
    "    creation_date datetime NOT NULL,\\\n",
    "    results longtext   NOT NULL,\\\n",
    "    PRIMARY KEY (id)\\\n",
    ") \")\n",
    "\n",
    "# Create test results table ( This Table will contains the results of the training and tests)\n",
    "mycursor.execute(\"CREATE TABLE testResults(\\\n",
    "    pipelineId INT,\\\n",
    "    nodename VARCHAR(100) NOT NULL, \\\n",
    "    confusionmatrix VARCHAR(255),\\\n",
    "    accuracy LONG,\\\n",
    "    sensivity LONG,\\\n",
    "    ppv LONG,\\\n",
    "    npv LONG,\\\n",
    "    f1score LONG,\\\n",
    "    fpr LONG,\\\n",
    "    tpr LONG, \\\n",
    "    PRIMARY KEY (pipelineId , nodename), \\\n",
    "    FOREIGN KEY (pipelineId) REFERENCES FLpipeline(id)\\\n",
    ") \")\n",
    "\n",
    "# Create FederatedDataset table\n",
    "mycursor.execute(\"CREATE TABLE FedDatasets (\\\n",
    "    FedId int NOT NULL AUTO_INCREMENT,\\\n",
    "    FLsetupId int DEFAULT NULL,\\\n",
    "    FLpipeId int DEFAULT NULL,\\\n",
    "    name varchar(255) NOT NULL,\\\n",
    "    PRIMARY KEY (FedId),\\\n",
    "    KEY FedDatasets_ibfk_1 (FLsetupId),\\\n",
    "    KEY FedDatasets_ibfk_2 (FLpipeId),\\\n",
    "    CONSTRAINT FedDatasets_ibfk_1 FOREIGN KEY (FLsetupId) REFERENCES FLsetup(FLsetupId) ON DELETE SET NULL ON UPDATE SET NULL,\\\n",
    "    CONSTRAINT FedDatasets_ibfk_2 FOREIGN KEY (FLpipeId) REFERENCES FLpipeline(id) ON DELETE SET NULL ON UPDATE SET NULL\\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit and save the changes \n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the created Tables \n",
    "mycursor.execute(\"SHOW TABLES\")\n",
    "tables = mycursor.fetchall()\n",
    "\n",
    "for table in tables:\n",
    "    print(table)  # Display the table names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00f76c",
   "metadata": {},
   "source": [
    "### Some UseCases of the dataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eng = create_engine('mysql+mysqlconnector://root:@localhost:3306/MEDfl')\n",
    "my_eng = my_eng.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56797306",
   "metadata": {},
   "source": [
    "#### Creating and Adding a Network to the Database\n",
    "\n",
    "In order to create a network, we'll use the `NetWork` class provided by *MedFl*. Instantiating this class requires providing the network's name. The creation process involves using the `create_network` method, which adds the newly created network to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eeeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the newtwork class with the name \n",
    "Net = Network(\"Net1\")\n",
    "# Create and add the newtwork to the dataBase\n",
    "Net.create_network()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6a6ca",
   "metadata": {},
   "source": [
    "#### List all created netWorks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f795b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Net.list_allnetworks()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0bc071",
   "metadata": {},
   "source": [
    "#### Creating a MasterDataSet for the Network\n",
    "\n",
    "Using the Methode `create_master_dataset` we will create a new table **MasterDataset**, and upload the data of `path_to_csv` to it, ( path_to_csv is optional and by default is the file specified on `Medfl.LearningManager.params.yaml` : `path_to_master_csv`  )\n",
    "The MasterDataSet serves dual purposes within the network:\n",
    "\n",
    "1. **Auto-Creation of Nodes:**\n",
    "    - When automatically creating nodes, the MasterDataSet plays a pivotal role in dividing the data across various train and test nodes.\n",
    "\n",
    "2. **Manual Creation of Nodes:**\n",
    "    - In scenarios involving manual creation of nodes, the MasterDataSet acts as a reference point to verify compatibility between different dataSets and the MasterDataSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masterDataSet Table\n",
    "Net.create_master_dataset() ; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae60ff4",
   "metadata": {},
   "source": [
    "#### Create nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56976c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiating the Node class \n",
    "node = Node(name = \"node3\" , train = 1)\n",
    "# Create the node \n",
    "node.create_node(NetId=1)\n",
    "\n",
    "# List all nodes \n",
    "nodeList = node.list_allnodes()\n",
    "nodeList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1747d",
   "metadata": {},
   "source": [
    "### Uploading DataSet to Nodes\n",
    "\n",
    "To upload a dataSet to a node directly, there are two options available:\n",
    "\n",
    "**Upload a New DataSet:** | **Using an Existing DataSet:**\n",
    "--------------------------|--------------------------------\n",
    "Use the `Node.upload_dataset` method. This method enables the direct upload of a CSV file to a node, adding it to the database, and assigning it to the respective node.|Utilize the `Node.affect_dataSet` method. This approach involves using an existing DataSet already present in the database and assigning it to the node. This option is viable if the DataSet was previously added to the database without being assigned to any node using the `DataSet.upload_dataset` method. It's also beneficial when reusing the same DataSet for another node in a different experiment, already associated with a node in a prior experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55fadd",
   "metadata": {},
   "source": [
    "1. **Upload a new DataSet**\n",
    "   \n",
    "   we will upload the dataset `eicu_test_1.csv` to the node that we have create **test_node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f293510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the data set to the node \n",
    "node.upload_dataset(dataset_name ='Test_Data_set' , path_to_csv='../eicu_test_1.csv') ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the node DataSet\n",
    "node_dataset = node.get_dataset() ; \n",
    "node_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918342be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all DataSets assotiated with the node\n",
    "data = node.list_alldatasets()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365458c",
   "metadata": {},
   "source": [
    "2. **Using an existing DataSet**\n",
    "   \n",
    "In this scenario, we'll use an existing dataSet and assign it to a node. However, before assigning it, we need to add the dataSet using `DataSet.upload_dataset()`. There's an optional parameter called *NodeId* that allows assigning the added dataset to a node. By default, this parameter is set to -1, indicating that the added dataset is not assigned to any node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2729c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv file\n",
    "path_to_csv = \"../eicu_test_2.csv\"\n",
    "#\n",
    "ds = DataSet(name=\"Dataset_3\", path=path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataSet with the default NodeId = - 1 ( not assined to any node )\n",
    "ds.upload_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6085f",
   "metadata": {},
   "source": [
    "List All DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allDatasets = ds.list_alldatasets(my_eng)\n",
    "allDatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c046873",
   "metadata": {},
   "source": [
    "Assign DataSet to Node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_2 = Node(name=\"node_2\" , train = 0 ) ; \n",
    "\n",
    "# # Create a new node \n",
    "# node_2.create_node(NetId=1) \n",
    "\n",
    "# assing Dataset_3 to node_2 \n",
    "node_2.assign_dataset(dataset_name=\"DataSet_3\"); \n",
    "\n",
    "# Display node Data Set\n",
    "data_2 = node_2.get_dataset() ; \n",
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95607d0",
   "metadata": {},
   "source": [
    "#### Unassigne DataSet \n",
    "\n",
    "The action of unassigning a dataSet is essentially the reverse or inverse of assigning a dataSet to a node. This process involves setting the nodeId to -1, which means disassociating the dataSet from any specific node. Consequently, when the nodeId is -1, the dataSet is not assigned to any node within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ee009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unAssigning DataSet_3\n",
    "node_2.unassign_dataset('DataSet_3') ; \n",
    "\n",
    "# Display node Data Set\n",
    "data_2 = node_2.get_dataset() ; \n",
    "data_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d47c9",
   "metadata": {},
   "source": [
    "#### Delete DataSet \n",
    "Deleting a dataSet involves the complete removal of all samples or records associated with that specific dataSet from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ebfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the DataSet Dataset_3 \n",
    "ds.delete_dataset() ; \n",
    "\n",
    "# List All available DataSets \n",
    "allDatasets = ds.list_alldatasets(my_eng)\n",
    "allDatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60f442",
   "metadata": {},
   "source": [
    "### Create Federated DataSet\n",
    "\n",
    "The Federated DataSet is a dataset used as a connector between the NetManager and the Learning Manager, \n",
    "To create a Federated DataSet we use the methode `create_federated_dataset` of the class `FLsetup`, it will go through all nodes,and generate **trainloders** & **valloaders** for the train nodes, and testloaders for the test nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cf46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from Medfl.NetManager import network, flsetup, net_helper\n",
    "reload(network)\n",
    "reload(flsetup)\n",
    "reload(net_helper)\n",
    "\n",
    "from Medfl.NetManager.network import Network\n",
    "from Medfl.NetManager.flsetup import FLsetup\n",
    "\n",
    "\n",
    "\n",
    "Net = Network('random')\n",
    "\n",
    "network = Net.use_network('Net1')\n",
    "\n",
    "# Create a FLsetup\n",
    "flSetup = FLsetup(name=\"My_FLSetUp\",\n",
    "                  description=\"This is just a test\", network=Net)\n",
    "\n",
    "\n",
    "# Create a FL DATASET\n",
    "# By default the take these values :\n",
    "# val_frac=0.1, test_frac=0.2\n",
    "flDataSet = flSetup.create_federated_dataset(\n",
    "    output=\"event_death\", \n",
    "    fill_strategy=\"mean\", \n",
    "    fit_encode=[\"site_hospital\", \"site_region\"], \n",
    "    to_drop=[\"DataSetId\", \"DataSetName\", \"id\", \"event_death\", \"NodeId\"], \n",
    "    val_frac=0.1, \n",
    "    test_frac=0.15)\n",
    "\n",
    "flDataSet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
