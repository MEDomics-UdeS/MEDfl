{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated-Transfer learning Tutorial − Integrating Transfer learning to Federated Learning using MEDFl package\n",
    "\n",
    "@Author : [MEDomics consortium](https://github.com/medomics/)\n",
    "\n",
    "@Email : medomics.info@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of integrating [Transfer learning](https://ieeexplore.ieee.org/abstract/document/5288526/) using the *MEDFl* package. The primary objective of incorporating transfer learning with the package is to harness the capabilities of [Federated-Transfer learning](https://link.springer.com/chapter/10.1007/978-3-031-11748-0_3) in training models across different hospitals. In real-world scenarios, one of the clients or the aggregating server might possess a [pre-trained model](https://blogs.nvidia.com/blog/what-is-a-pretrained-ai-model/#:~:text=A%20pretrained%20AI%20model%20is,8%2C%202022%20by%20Angie%20Lee). Leveraging this pre-trained model offers advantages such as enhancing performance and reducing training time.\n",
    "\n",
    "In some instances, a client may lack sufficient data to train a model entirely from scratch, hindering the ability to achieve optimal performance. Utilizing transfer learning becomes a viable strategy to maximize the benefits from each client, allowing the integration of previously learned knowledge to enhance model training and performance.\n",
    "\n",
    "<img src=\"../Images/FTL_comp.png\"  style=\"width:600px ;height:400px ; display:block ;margin:0 auto\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EiCu Data \n",
    "This tutorial involves the utilization of the eICU dataset, a CSV file contains information on 200,860 patients, to train a binary classifier model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Start\n",
    "\n",
    "To integrate Transfer Learning into the *MEDFl* package, several sequential steps are essential:\n",
    "\n",
    "1. **Importing a Pretrained Model:** Acquire or utilize a pretrained model containing learned knowledge from prior tasks or domains.\n",
    "2. **Initialization at the Central Server:** The central server initiates its model by copying the pretrained model's parameters or weights.\n",
    "3. **Initiating Federated Learning:** Upon initialization, the federated learning process begins, facilitating the exchange of model updates among participating clients for joint model training.\n",
    "\n",
    "### Importing a Pretrained Model:\n",
    "   - When importing a pretrained model, there are two options available:\n",
    "   \n",
    "     1. **Train and Save Locally:** Train a model localy, save it, and subsequently incorporate it for use.\n",
    "     2. **External Source or Previous Work:** Import a pretrained model from an external source or retrieve it from a previous project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train and Save Localy\n",
    "In this section, we aim to train a basic binary classifier model using the `eicu_sapsii_data.csv` dataset. Following the model training, we'll save the trained model for future utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '../..'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientunitstayid</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hospitalid</th>\n",
       "      <th>deceased</th>\n",
       "      <th>age</th>\n",
       "      <th>stay_length</th>\n",
       "      <th>organdonor</th>\n",
       "      <th>bicarbonate_min</th>\n",
       "      <th>bicarbonate_max</th>\n",
       "      <th>...</th>\n",
       "      <th>uo</th>\n",
       "      <th>aids</th>\n",
       "      <th>hem</th>\n",
       "      <th>mets</th>\n",
       "      <th>admissiontype</th>\n",
       "      <th>sepsis3</th>\n",
       "      <th>unitvisitnumber</th>\n",
       "      <th>unitvisitnumberrevised</th>\n",
       "      <th>num_admiss</th>\n",
       "      <th>num_hosp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141650</td>\n",
       "      <td>129307</td>\n",
       "      <td>002-17293</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>5787</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141777</td>\n",
       "      <td>129401</td>\n",
       "      <td>002-44088</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1559</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141907</td>\n",
       "      <td>129494</td>\n",
       "      <td>002-60303</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1135</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2380.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141939</td>\n",
       "      <td>129522</td>\n",
       "      <td>002-66368</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>448</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141978</td>\n",
       "      <td>129552</td>\n",
       "      <td>002-25450</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>1104</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patientunitstayid  hadm_id subject_id  hospitalid  deceased  age  \\\n",
       "0             141650   129307  002-17293          73         0   71   \n",
       "1             141777   129401  002-44088          73         0   60   \n",
       "2             141907   129494  002-60303          63         0   68   \n",
       "3             141939   129522  002-66368          69         0   56   \n",
       "4             141978   129552  002-25450          67         0   77   \n",
       "\n",
       "   stay_length  organdonor  bicarbonate_min  bicarbonate_max  ...      uo  \\\n",
       "0         5787           0             23.0             28.0  ...     NaN   \n",
       "1         1559           0             27.0             28.0  ...     NaN   \n",
       "2         1135           0             23.0             23.0  ...  2380.0   \n",
       "3          448           0              NaN              NaN  ...    50.0   \n",
       "4         1104           0             23.0             30.0  ...     NaN   \n",
       "\n",
       "   aids  hem  mets  admissiontype  sepsis3  unitvisitnumber  \\\n",
       "0   NaN  NaN   NaN            6.0        0                1   \n",
       "1   0.0  0.0   0.0            6.0        0                1   \n",
       "2   NaN  NaN   NaN            6.0        0                1   \n",
       "3   0.0  0.0   0.0            6.0        0                1   \n",
       "4   0.0  0.0   0.0            0.0        0                1   \n",
       "\n",
       "   unitvisitnumberrevised  num_admiss  num_hosp  \n",
       "0                       1           1         1  \n",
       "1                       1           1         1  \n",
       "2                       1           1         1  \n",
       "3                       1           1         1  \n",
       "4                       1           2         2  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv('D:\\ESI\\\\3CS\\PFE\\last_year\\Code\\MEDfl\\\\notebooks\\data\\eicu_sapsii_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing Steps\n",
    "\n",
    "Here, we perform several essential data preprocessing steps:\n",
    "\n",
    "1. **Drop Unnecessary Columns:** The code drops the 'subject_id' column and any other columns deemed unnecessary for the analysis. Additional columns can be added to the `columns_to_drop` list for removal.\n",
    "\n",
    "2. **Define Features and Target Variable:** The features are defined by selecting all columns except the 'deceased' column, which is designated as the target variable for the binary classification task.\n",
    "\n",
    "3. **Impute Missing Values:** Missing values in the selected features are imputed using the mean strategy. The `SimpleImputer` from the scikit-learn library is employed to fill missing values in the dataset.\n",
    "\n",
    "4. **Preview the Transformed Data:** The `.head()` function is used to display the first few rows of the transformed dataset after preprocessing.\n",
    "\n",
    "The code snippet provides a glimpse of the preprocessing steps, ensuring data cleanliness and preparation for training the binary classifier using the eICU dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientunitstayid</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>hospitalid</th>\n",
       "      <th>deceased</th>\n",
       "      <th>age</th>\n",
       "      <th>stay_length</th>\n",
       "      <th>organdonor</th>\n",
       "      <th>bicarbonate_min</th>\n",
       "      <th>bicarbonate_max</th>\n",
       "      <th>bilirubin_min</th>\n",
       "      <th>...</th>\n",
       "      <th>uo</th>\n",
       "      <th>aids</th>\n",
       "      <th>hem</th>\n",
       "      <th>mets</th>\n",
       "      <th>admissiontype</th>\n",
       "      <th>sepsis3</th>\n",
       "      <th>unitvisitnumber</th>\n",
       "      <th>unitvisitnumberrevised</th>\n",
       "      <th>num_admiss</th>\n",
       "      <th>num_hosp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141650.0</td>\n",
       "      <td>129307.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5787.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>1571.13745</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141777.0</td>\n",
       "      <td>129401.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>...</td>\n",
       "      <td>1571.13745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141907.0</td>\n",
       "      <td>129494.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>...</td>\n",
       "      <td>2380.00000</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141939.0</td>\n",
       "      <td>129522.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.223356</td>\n",
       "      <td>25.568875</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>...</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141978.0</td>\n",
       "      <td>129552.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1571.13745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patientunitstayid   hadm_id  hospitalid  deceased   age  stay_length  \\\n",
       "0           141650.0  129307.0        73.0         0  71.0       5787.0   \n",
       "1           141777.0  129401.0        73.0         0  60.0       1559.0   \n",
       "2           141907.0  129494.0        63.0         0  68.0       1135.0   \n",
       "3           141939.0  129522.0        69.0         0  56.0        448.0   \n",
       "4           141978.0  129552.0        67.0         0  77.0       1104.0   \n",
       "\n",
       "   organdonor  bicarbonate_min  bicarbonate_max  bilirubin_min  ...  \\\n",
       "0         0.0        23.000000        28.000000       0.900000  ...   \n",
       "1         0.0        27.000000        28.000000       0.970553  ...   \n",
       "2         0.0        23.000000        23.000000       0.970553  ...   \n",
       "3         0.0        23.223356        25.568875       0.970553  ...   \n",
       "4         0.0        23.000000        30.000000       0.500000  ...   \n",
       "\n",
       "           uo      aids       hem      mets  admissiontype  sepsis3  \\\n",
       "0  1571.13745  0.003397  0.008301  0.009052            6.0      0.0   \n",
       "1  1571.13745  0.000000  0.000000  0.000000            6.0      0.0   \n",
       "2  2380.00000  0.003397  0.008301  0.009052            6.0      0.0   \n",
       "3    50.00000  0.000000  0.000000  0.000000            6.0      0.0   \n",
       "4  1571.13745  0.000000  0.000000  0.000000            0.0      0.0   \n",
       "\n",
       "   unitvisitnumber  unitvisitnumberrevised  num_admiss  num_hosp  \n",
       "0              1.0                     1.0         1.0       1.0  \n",
       "1              1.0                     1.0         1.0       1.0  \n",
       "2              1.0                     1.0         1.0       1.0  \n",
       "3              1.0                     1.0         1.0       1.0  \n",
       "4              1.0                     1.0         2.0       2.0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop 'subject_id' column and any other unnecessary columns\n",
    "columns_to_drop = ['subject_id']  # Add more columns to drop if needed\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Define features and target variable\n",
    "features = [col for col in data.columns if col != 'deceased']\n",
    "target = 'deceased'\n",
    "\n",
    "# Impute missing values using the mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[features] = imputer.fit_transform(data[features])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modal initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.23355887793390245\n",
      "Epoch 2/20 - Loss: 0.21586696147081957\n",
      "Epoch 3/20 - Loss: 0.2118789857366185\n",
      "Epoch 4/20 - Loss: 0.21033351138688117\n",
      "Epoch 5/20 - Loss: 0.20836161097718608\n",
      "Epoch 6/20 - Loss: 0.20733126593113527\n",
      "Epoch 7/20 - Loss: 0.20663039093857172\n",
      "Epoch 8/20 - Loss: 0.20626463453359203\n",
      "Epoch 9/20 - Loss: 0.2054598361586804\n",
      "Epoch 10/20 - Loss: 0.20532208540425304\n",
      "Epoch 11/20 - Loss: 0.2040670669401359\n",
      "Epoch 12/20 - Loss: 0.20481527690474896\n",
      "Epoch 13/20 - Loss: 0.20436784797854357\n",
      "Epoch 14/20 - Loss: 0.20448091222445822\n",
      "Epoch 15/20 - Loss: 0.20409286246133937\n",
      "Epoch 16/20 - Loss: 0.2043396927555782\n",
      "Epoch 17/20 - Loss: 0.20391304912961714\n",
      "Epoch 18/20 - Loss: 0.20326222188189205\n",
      "Epoch 19/20 - Loss: 0.20368710530262363\n",
      "Epoch 20/20 - Loss: 0.20328472280294024\n",
      "Test Accuracy: 0.93\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.99      0.96     36462\n",
      "         1.0       0.82      0.24      0.38      3710\n",
      "\n",
      "    accuracy                           0.93     40172\n",
      "   macro avg       0.88      0.62      0.67     40172\n",
      "weighted avg       0.92      0.93      0.91     40172\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36268   194]\n",
      " [ 2808   902]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network model using PyTorch\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = BinaryClassifier(input_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare data loaders for PyTorch\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modal Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 20\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predictions = (outputs.squeeze() > 0.5).float()  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_tensor.numpy(), predictions.numpy())\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_tensor.numpy(), predictions.numpy()))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_tensor.numpy(), predictions.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Trained modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trainedModels/binary_classifier_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Pre-trained Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (fc1): Linear(in_features=38, out_features=64, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model (same architecture as before)\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = BinaryClassifier(input_dim)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model.load_state_dict(torch.load('trainedModels/binary_classifier_model.pth'))\n",
    "\n",
    "# Ensure the model is in evaluation mode for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Pretrained Model\n",
    "\n",
    "Following the initial training of the model, several modifications can be applied to adapt the pretrained model to the new context of the federated learning problem. Various options for modifying the pretrained model include:\n",
    "\n",
    "1. **Fine-Tuning Layers:** Selective updating of specific layers allows fine-tuning the model. Lower-level layers may retain learned representations, while higher-level layers adapt to the new task or domain.\n",
    "\n",
    "2. **Adjusting Learning Rates:** Modification of learning rates for different layers or layer groups aids in emphasizing specific layers' importance or controlling updates to pretrained layers.\n",
    "\n",
    "3. **Changing Activation Functions:** Experimentation with different activation functions in certain layers influences the model's learning behavior and adaptability to new data.\n",
    "\n",
    "4. **Transfer Learning Strategies:** Employing strategies like feature extraction or full retraining based on available data and computational resources to leverage pretrained model knowledge.\n",
    "\n",
    "5. **Customized Loss Functions:** Designing or utilizing domain-specific loss functions tailored to address specific requirements of the federated learning problem.\n",
    "\n",
    "These modification options provide flexibility and customization in adapting the pretrained model, ensuring its alignment with the unique demands and characteristics of the federated learning or transfer learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fine-Tuning Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, fine-tune the last layers of the pretrained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "for param in model.output.parameters():\n",
    "    param.requires_grad = True  # Unfreeze output layer for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Adjusting Learning Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different learning rates for different layer groups\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.fc1.parameters(), 'lr': 0.0001},  # Lower LR for certain layers\n",
    "    {'params': model.fc2.parameters(), 'lr': 0.0005},  # Higher LR for certain layers\n",
    "    {'params': model.output.parameters(), 'lr': 0.001}  # LR for output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Changing Activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different activation functions in certain layers\n",
    "# For instance, using LeakyReLU in place of ReLU\n",
    "model.fc1 = nn.Linear(input_dim, 64)\n",
    "model.fc2 = nn.Linear(64, 32)\n",
    "model.output = nn.Linear(32, 1)\n",
    "model.leaky_relu = nn.LeakyReLU()  # Introduce LeakyReLU activation function\n",
    "model.sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally\n",
    "After having the pretrained modal ready to use we have to pass it the Server class of the `MEDFl` package so the federated learing process can start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
